{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CARTE_IDENTITE_PASSEPORT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "from langflow.load import upload_file\n",
    "import requests\n",
    "\n",
    "LANGFLOW_API_URL = \"http://127.0.0.1:7860\"\n",
    "\n",
    "def parse_llm_output(text):\n",
    "    try:\n",
    "        # Try parsing as JSON with quotes\n",
    "        parsed_text = json.loads(text)\n",
    "        return parsed_text\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # Try parsing as Python dict string\n",
    "            cleaned_text = text.replace(\"`\", '')\n",
    "            cleaned_text = cleaned_text.replace(\"json\", '')\n",
    "            parsed_text = json.loads(cleaned_text)\n",
    "            return parsed_text\n",
    "        except json.JSONDecodeError:\n",
    "            # Return None if parsing fails\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_flow(id: str):\n",
    "    url = f\"{LANGFLOW_API_URL}/api/v1/flows/{id}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_node(edges:list[dict], node: str):\n",
    "    matches = [edge.get('source') for edge in edges if edge.get('source').startswith(node)]\n",
    "    if len(matches):\n",
    "        return matches.pop()\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_flow(flow_id: str, ocr_file_path:Optional[str]=None):\n",
    "\n",
    "    payload: dict = {}\n",
    "    TWEAKS = {\n",
    "    }\n",
    "    \n",
    "    flow = get_flow(flow_id)\n",
    "    edges = flow.get('data').get('edges')\n",
    "\n",
    "    ocr_node = get_node(edges, \"DemarchesSimplifieesOCR\")\n",
    "    if ocr_node:\n",
    "        payload_with_file = upload_file(\n",
    "            file_path=ocr_file_path,\n",
    "            host=LANGFLOW_API_URL,\n",
    "            flow_id=flow_id,\n",
    "            components=[\"File-ID\"],\n",
    "            tweaks=payload,\n",
    "        )\n",
    "        TWEAKS[ocr_node] = {\n",
    "            \"document\": payload_with_file.get('File-ID').get('path')\n",
    "        }\n",
    "\n",
    "    albert_node = get_node(edges, \"AlbertModel\")\n",
    "    if albert_node:\n",
    "        TWEAKS[albert_node] = {\n",
    "            \"api_key\": os.environ.get(\"ALBERT_API\")\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(\n",
    "        f\"{LANGFLOW_API_URL}/api/v1/run/{flow_id}\",\n",
    "        json={\"tweaks\": TWEAKS},\n",
    "        headers=headers\n",
    "    )\n",
    "    data = json.loads(response.content)\n",
    "    try:\n",
    "        text_content = data['outputs'][0]['outputs'][0]['results']['message']['data']['text']\n",
    "        parsed_text = parse_llm_output(text_content)\n",
    "        category = parsed_text.get(\"category\", None)\n",
    "        return category\n",
    "    except Exception as e:        \n",
    "        return \"FAILED\"\n",
    "\n",
    "\n",
    "\n",
    "ALBERT_FLOW = \"d3d21488-2924-4f2a-9d03-2e696c48d7cc\"\n",
    "OLLAMA_FLOW = \"f71bcc8b-9cc0-455e-9ddc-15811164b8a3\"\n",
    "\n",
    "# file_path = \"./data/carte_grise.jpg\"\n",
    "# file_path = \"./data/rib_iban_erwan_boehm.pdf\"\n",
    "file_path = \"./data/cni.jpg\"\n",
    "\n",
    "\n",
    "run_flow(ALBERT_FLOW, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Document Q&A',\n",
       " 'description': 'Integrates PDF reading with a language model to answer document-specific questions. Ideal for small-scale texts, it facilitates direct queries with immediate insights.',\n",
       " 'icon': 'FileQuestion',\n",
       " 'icon_bg_color': None,\n",
       " 'gradient': '3',\n",
       " 'data': {'nodes': [{'data': {'description': 'Display a chat message in the Playground.',\n",
       "     'display_name': 'Chat Output',\n",
       "     'id': 'ChatOutput-ECs31',\n",
       "     'node': {'base_classes': ['Message'],\n",
       "      'beta': False,\n",
       "      'conditional_paths': [],\n",
       "      'custom_fields': {},\n",
       "      'description': 'Display a chat message in the Playground.',\n",
       "      'display_name': 'Chat Output',\n",
       "      'documentation': '',\n",
       "      'edited': False,\n",
       "      'field_order': ['input_value',\n",
       "       'should_store_message',\n",
       "       'sender',\n",
       "       'sender_name',\n",
       "       'session_id',\n",
       "       'data_template',\n",
       "       'background_color',\n",
       "       'chat_icon',\n",
       "       'text_color'],\n",
       "      'frozen': False,\n",
       "      'icon': 'MessagesSquare',\n",
       "      'legacy': False,\n",
       "      'lf_version': '1.1.3',\n",
       "      'metadata': {},\n",
       "      'output_types': [],\n",
       "      'outputs': [{'allows_loop': False,\n",
       "        'cache': True,\n",
       "        'display_name': 'Message',\n",
       "        'method': 'message_response',\n",
       "        'name': 'message',\n",
       "        'selected': 'Message',\n",
       "        'types': ['Message'],\n",
       "        'value': '__UNDEFINED__'}],\n",
       "      'pinned': False,\n",
       "      'template': {'_type': 'Component',\n",
       "       'background_color': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Background Color',\n",
       "        'dynamic': False,\n",
       "        'info': 'The background color of the icon.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'background_color',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''},\n",
       "       'chat_icon': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Icon',\n",
       "        'dynamic': False,\n",
       "        'info': 'The icon of the message.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'chat_icon',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''},\n",
       "       'code': {'advanced': True,\n",
       "        'dynamic': True,\n",
       "        'fileTypes': [],\n",
       "        'file_path': '',\n",
       "        'info': '',\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'multiline': True,\n",
       "        'name': 'code',\n",
       "        'password': False,\n",
       "        'placeholder': '',\n",
       "        'required': True,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'type': 'code',\n",
       "        'value': 'from langflow.base.io.chat import ChatComponent\\nfrom langflow.inputs import BoolInput\\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\\nfrom langflow.schema.message import Message\\nfrom langflow.schema.properties import Source\\nfrom langflow.utils.constants import (\\n    MESSAGE_SENDER_AI,\\n    MESSAGE_SENDER_NAME_AI,\\n    MESSAGE_SENDER_USER,\\n)\\n\\n\\nclass ChatOutput(ChatComponent):\\n    display_name = \"Chat Output\"\\n    description = \"Display a chat message in the Playground.\"\\n    icon = \"MessagesSquare\"\\n    name = \"ChatOutput\"\\n    minimized = True\\n\\n    inputs = [\\n        MessageInput(\\n            name=\"input_value\",\\n            display_name=\"Text\",\\n            info=\"Message to be passed as output.\",\\n        ),\\n        BoolInput(\\n            name=\"should_store_message\",\\n            display_name=\"Store Messages\",\\n            info=\"Store the message in the history.\",\\n            value=True,\\n            advanced=True,\\n        ),\\n        DropdownInput(\\n            name=\"sender\",\\n            display_name=\"Sender Type\",\\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\\n            value=MESSAGE_SENDER_AI,\\n            advanced=True,\\n            info=\"Type of sender.\",\\n        ),\\n        MessageTextInput(\\n            name=\"sender_name\",\\n            display_name=\"Sender Name\",\\n            info=\"Name of the sender.\",\\n            value=MESSAGE_SENDER_NAME_AI,\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"session_id\",\\n            display_name=\"Session ID\",\\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"data_template\",\\n            display_name=\"Data Template\",\\n            value=\"{text}\",\\n            advanced=True,\\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data\\'s text key.\",\\n        ),\\n        MessageTextInput(\\n            name=\"background_color\",\\n            display_name=\"Background Color\",\\n            info=\"The background color of the icon.\",\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"chat_icon\",\\n            display_name=\"Icon\",\\n            info=\"The icon of the message.\",\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"text_color\",\\n            display_name=\"Text Color\",\\n            info=\"The text color of the name\",\\n            advanced=True,\\n        ),\\n    ]\\n    outputs = [\\n        Output(\\n            display_name=\"Message\",\\n            name=\"message\",\\n            method=\"message_response\",\\n        ),\\n    ]\\n\\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\\n        source_dict = {}\\n        if id_:\\n            source_dict[\"id\"] = id_\\n        if display_name:\\n            source_dict[\"display_name\"] = display_name\\n        if source:\\n            source_dict[\"source\"] = source\\n        return Source(**source_dict)\\n\\n    async def message_response(self) -> Message:\\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\\n        background_color = self.background_color\\n        text_color = self.text_color\\n        if self.chat_icon:\\n            icon = self.chat_icon\\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\\n        message.sender = self.sender\\n        message.sender_name = self.sender_name\\n        message.session_id = self.session_id\\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\\n        message.properties.source = self._build_source(source_id, display_name, source)\\n        message.properties.icon = icon\\n        message.properties.background_color = background_color\\n        message.properties.text_color = text_color\\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\\n            stored_message = await self.send_message(\\n                message,\\n            )\\n            self.message.value = stored_message\\n            message = stored_message\\n\\n        self.status = message\\n        return message\\n'},\n",
       "       'data_template': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Data Template',\n",
       "        'dynamic': False,\n",
       "        'info': \"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'data_template',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': '{text}'},\n",
       "       'input_value': {'_input_type': 'MessageInput',\n",
       "        'advanced': False,\n",
       "        'display_name': 'Text',\n",
       "        'dynamic': False,\n",
       "        'info': 'Message to be passed as output.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'input_value',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''},\n",
       "       'sender': {'_input_type': 'DropdownInput',\n",
       "        'advanced': True,\n",
       "        'combobox': False,\n",
       "        'display_name': 'Sender Type',\n",
       "        'dynamic': False,\n",
       "        'info': 'Type of sender.',\n",
       "        'name': 'sender',\n",
       "        'options': ['Machine', 'User'],\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': 'Machine'},\n",
       "       'sender_name': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Sender Name',\n",
       "        'dynamic': False,\n",
       "        'info': 'Name of the sender.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'sender_name',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': 'AI'},\n",
       "       'session_id': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Session ID',\n",
       "        'dynamic': False,\n",
       "        'info': 'The session ID of the chat. If empty, the current session ID parameter will be used.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'session_id',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''},\n",
       "       'should_store_message': {'_input_type': 'BoolInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Store Messages',\n",
       "        'dynamic': False,\n",
       "        'info': 'Store the message in the history.',\n",
       "        'list': False,\n",
       "        'name': 'should_store_message',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'bool',\n",
       "        'value': True},\n",
       "       'text_color': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Text Color',\n",
       "        'dynamic': False,\n",
       "        'info': 'The text color of the name',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'text_color',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''}},\n",
       "      'tool_mode': False},\n",
       "     'type': 'ChatOutput'},\n",
       "    'dragging': False,\n",
       "    'height': 234,\n",
       "    'id': 'ChatOutput-ECs31',\n",
       "    'measured': {'height': 234, 'width': 320},\n",
       "    'position': {'x': 1649.9976202514392, 'y': -83.71360634912952},\n",
       "    'positionAbsolute': {'x': 1631.3766926569258, 'y': 136.66509468115308},\n",
       "    'selected': True,\n",
       "    'type': 'genericNode',\n",
       "    'width': 320},\n",
       "   {'data': {'description': 'Create a prompt template with dynamic variables.',\n",
       "     'display_name': 'Prompt',\n",
       "     'id': 'Prompt-71BBi',\n",
       "     'node': {'template': {'_type': 'Component',\n",
       "       'code': {'advanced': True,\n",
       "        'dynamic': True,\n",
       "        'fileTypes': [],\n",
       "        'file_path': '',\n",
       "        'info': '',\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'multiline': True,\n",
       "        'name': 'code',\n",
       "        'password': False,\n",
       "        'placeholder': '',\n",
       "        'required': True,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'type': 'code',\n",
       "        'value': 'from langflow.base.prompts.api_utils import process_prompt_template\\nfrom langflow.custom import Component\\nfrom langflow.inputs.inputs import DefaultPromptField\\nfrom langflow.io import MessageTextInput, Output, PromptInput\\nfrom langflow.schema.message import Message\\nfrom langflow.template.utils import update_template_values\\n\\n\\nclass PromptComponent(Component):\\n    display_name: str = \"Prompt\"\\n    description: str = \"Create a prompt template with dynamic variables.\"\\n    icon = \"prompts\"\\n    trace_type = \"prompt\"\\n    name = \"Prompt\"\\n\\n    inputs = [\\n        PromptInput(name=\"template\", display_name=\"Template\"),\\n        MessageTextInput(\\n            name=\"tool_placeholder\",\\n            display_name=\"Tool Placeholder\",\\n            tool_mode=True,\\n            advanced=True,\\n            info=\"A placeholder input for tool mode.\",\\n        ),\\n    ]\\n\\n    outputs = [\\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\\n    ]\\n\\n    async def build_prompt(self) -> Message:\\n        prompt = Message.from_template(**self._attributes)\\n        self.status = prompt.text\\n        return prompt\\n\\n    def _update_template(self, frontend_node: dict):\\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\\n        custom_fields = frontend_node[\"custom_fields\"]\\n        frontend_node_template = frontend_node[\"template\"]\\n        _ = process_prompt_template(\\n            template=prompt_template,\\n            name=\"template\",\\n            custom_fields=custom_fields,\\n            frontend_node_template=frontend_node_template,\\n        )\\n        return frontend_node\\n\\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\\n        \"\"\"This function is called after the code validation is done.\"\"\"\\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\\n        # Kept it duplicated for backwards compatibility\\n        _ = process_prompt_template(\\n            template=template,\\n            name=\"template\",\\n            custom_fields=frontend_node[\"custom_fields\"],\\n            frontend_node_template=frontend_node[\"template\"],\\n        )\\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\\n        # and update the frontend_node with those values\\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\\n        return frontend_node\\n\\n    def _get_fallback_input(self, **kwargs):\\n        return DefaultPromptField(**kwargs)\\n'},\n",
       "       'template': {'advanced': False,\n",
       "        'display_name': 'Template',\n",
       "        'dynamic': False,\n",
       "        'info': '',\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'template',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'trace_as_input': True,\n",
       "        'type': 'prompt',\n",
       "        'value': 'Ton but est de classifier le document suivant en fonction de son contenu.  \\nRéponds uniquement en JSON, sans explication ni texte supplémentaire.  \\n\\n### **Catégories possibles** :  \\n- **ATTESTATION_AFFILIATION_URSSAF** : Attestation officielle d\\'affiliation à l\\'URSSAF, mentionnant l\\'affiliation d\\'une entreprise ou d\\'un indépendant. Recherchez des termes comme \"Attestation d\\'affiliation\", \"URSSAF\", \"cotisations sociales\", \"numéro SIRET\".  \\n- **RIB** : Document contenant des informations bancaires comme un IBAN, un BIC, un numéro de compte ou le titre \"Relevé d\\'Identité Bancaire\".  \\n- **CARTE_IDENTITE_PASSEPORT** : Document officiel d\\'identité comme une carte nationale d’identité ou un passeport. Recherchez \"Carte nationale d\\'identité\", \"Passeport\", une photo d\\'identité et des champs comme \"Nom\", \"Prénom\", \"Date de naissance\".  \\n- **JUSTIFICATIF_DOMICILE** : Facture ou attestation prouvant le domicile (facture EDF, téléphone, quittance de loyer, attestation d’hébergement). Recherchez \"Adresse\", \"Facture\", \"Quittance\", \"EDF\", \"Free\", \"Orange\", etc.  \\n- **EXTRAIT_ACTE_NAISSANCE** : Document officiel d’état civil contenant les informations de naissance d’une personne. Recherchez \"Extrait d’acte de naissance\", \"Mairie\", \"Naissance de\".  \\n- **AVIS_IMPOSITION** : Document fiscal émis par l’administration fiscale, indiquant les revenus et l’impôt à payer. Recherchez \"Avis d\\'imposition\", \"Impôt sur le revenu\", \"Direction générale des finances publiques (DGFIP)\".  \\n- **FICHE_DE_PAIE** : Bulletin de salaire contenant des informations sur le revenu et les cotisations sociales. Recherchez \"Salaire brut\", \"Salaire net\", \"Cotisations sociales\", \"Employeur\".  \\n- **PERMIS_DE_CONDUIRE** : Document officiel attestant du droit de conduire. Recherchez \"Permis de conduire\", une photo d’identité et des champs comme \"Nom\", \"Prénom\", \"Catégorie\".  \\n- **INCONNU** : Si le document ne correspond à aucune catégorie ci-dessus.  \\n- **CARTE_GRISE** : Certificat d’immatriculation d’un véhicule, mentionnant des informations comme le numéro d’immatriculation, le titulaire du véhicule et les caractéristiques du véhicule. Recherchez des termes comme \"Certificat d’immatriculation\", \"Immatriculation\", \"Carte grise\", \"Titulaire\", \"D.1\", \"D.2\", \"D.3\", \"E\".\\n\\nVoici le document\\n---\\n{document}\\n---\\n\\n\\nQuel est sa catégorie?'},\n",
       "       'tool_placeholder': {'_input_type': 'MessageTextInput',\n",
       "        'advanced': True,\n",
       "        'display_name': 'Tool Placeholder',\n",
       "        'dynamic': False,\n",
       "        'info': 'A placeholder input for tool mode.',\n",
       "        'input_types': ['Message'],\n",
       "        'list': False,\n",
       "        'load_from_db': False,\n",
       "        'name': 'tool_placeholder',\n",
       "        'placeholder': '',\n",
       "        'required': False,\n",
       "        'show': True,\n",
       "        'title_case': False,\n",
       "        'tool_mode': True,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'type': 'str',\n",
       "        'value': ''},\n",
       "       'document': {'field_type': 'str',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'list': False,\n",
       "        'show': True,\n",
       "        'multiline': True,\n",
       "        'value': '',\n",
       "        'fileTypes': [],\n",
       "        'file_path': '',\n",
       "        'name': 'document',\n",
       "        'display_name': 'document',\n",
       "        'advanced': False,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': '',\n",
       "        'load_from_db': False,\n",
       "        'title_case': False,\n",
       "        'type': 'str'}},\n",
       "      'description': 'Create a prompt template with dynamic variables.',\n",
       "      'icon': 'prompts',\n",
       "      'is_input': None,\n",
       "      'is_output': None,\n",
       "      'is_composition': None,\n",
       "      'base_classes': ['Message'],\n",
       "      'name': '',\n",
       "      'display_name': 'Prompt',\n",
       "      'documentation': '',\n",
       "      'minimized': False,\n",
       "      'custom_fields': {'template': ['document']},\n",
       "      'output_types': [],\n",
       "      'full_path': None,\n",
       "      'pinned': False,\n",
       "      'conditional_paths': [],\n",
       "      'frozen': False,\n",
       "      'outputs': [{'types': ['Message'],\n",
       "        'selected': 'Message',\n",
       "        'name': 'prompt',\n",
       "        'hidden': None,\n",
       "        'display_name': 'Prompt Message',\n",
       "        'method': 'build_prompt',\n",
       "        'value': '__UNDEFINED__',\n",
       "        'cache': True,\n",
       "        'required_inputs': None,\n",
       "        'allows_loop': False}],\n",
       "      'field_order': ['template'],\n",
       "      'beta': False,\n",
       "      'legacy': False,\n",
       "      'error': None,\n",
       "      'edited': False,\n",
       "      'metadata': {},\n",
       "      'tool_mode': False,\n",
       "      'lf_version': '1.1.3'},\n",
       "     'type': 'Prompt'},\n",
       "    'dragging': False,\n",
       "    'height': 347,\n",
       "    'id': 'Prompt-71BBi',\n",
       "    'measured': {'height': 347, 'width': 320},\n",
       "    'position': {'x': 863.903393839348, 'y': -307.8159561962758},\n",
       "    'positionAbsolute': {'x': 895.1947781377585, 'y': -59.89409263992732},\n",
       "    'selected': False,\n",
       "    'type': 'genericNode',\n",
       "    'width': 320},\n",
       "   {'id': 'OllamaModel-JEWib',\n",
       "    'type': 'genericNode',\n",
       "    'position': {'x': 1260.121635963691, 'y': -496.4515701367944},\n",
       "    'data': {'node': {'template': {'_type': 'Component',\n",
       "       'base_url': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'base_url',\n",
       "        'value': 'http://localhost:11434',\n",
       "        'display_name': 'Base URL',\n",
       "        'advanced': False,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'Endpoint of the Ollama API.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'code': {'type': 'code',\n",
       "        'required': True,\n",
       "        'placeholder': '',\n",
       "        'list': False,\n",
       "        'show': True,\n",
       "        'multiline': True,\n",
       "        'value': 'from typing import Any\\nfrom urllib.parse import urljoin\\n\\nimport httpx\\nfrom langchain_ollama import ChatOllama\\n\\nfrom langflow.base.models.model import LCModelComponent\\nfrom langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, OLLAMA_TOOL_MODELS_BASE, URL_LIST\\nfrom langflow.field_typing import LanguageModel\\nfrom langflow.field_typing.range_spec import RangeSpec\\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SliderInput\\n\\nHTTP_STATUS_OK = 200\\n\\n\\nclass ChatOllamaComponent(LCModelComponent):\\n    display_name = \"Ollama\"\\n    description = \"Generate text using Ollama Local LLMs.\"\\n    icon = \"Ollama\"\\n    name = \"OllamaModel\"\\n\\n    inputs = [\\n        MessageTextInput(\\n            name=\"base_url\",\\n            display_name=\"Base URL\",\\n            info=\"Endpoint of the Ollama API.\",\\n            value=\"\",\\n        ),\\n        DropdownInput(\\n            name=\"model_name\",\\n            display_name=\"Model Name\",\\n            options=[],\\n            info=\"Refer to https://ollama.com/library for more models.\",\\n            refresh_button=True,\\n            real_time_refresh=True,\\n        ),\\n        SliderInput(\\n            name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01)\\n        ),\\n        MessageTextInput(\\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\\n        ),\\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\\n        DropdownInput(\\n            name=\"mirostat\",\\n            display_name=\"Mirostat\",\\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\\n            value=\"Disabled\",\\n            advanced=True,\\n            real_time_refresh=True,\\n        ),\\n        FloatInput(\\n            name=\"mirostat_eta\",\\n            display_name=\"Mirostat Eta\",\\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\\n            advanced=True,\\n        ),\\n        FloatInput(\\n            name=\"mirostat_tau\",\\n            display_name=\"Mirostat Tau\",\\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\\n            advanced=True,\\n        ),\\n        IntInput(\\n            name=\"num_ctx\",\\n            display_name=\"Context Window Size\",\\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\\n            advanced=True,\\n        ),\\n        IntInput(\\n            name=\"num_gpu\",\\n            display_name=\"Number of GPUs\",\\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\\n            advanced=True,\\n        ),\\n        IntInput(\\n            name=\"num_thread\",\\n            display_name=\"Number of Threads\",\\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\\n            advanced=True,\\n        ),\\n        IntInput(\\n            name=\"repeat_last_n\",\\n            display_name=\"Repeat Last N\",\\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\\n            advanced=True,\\n        ),\\n        FloatInput(\\n            name=\"repeat_penalty\",\\n            display_name=\"Repeat Penalty\",\\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\\n            advanced=True,\\n        ),\\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\\n        IntInput(\\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\\n        ),\\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\", advanced=True),\\n        MessageTextInput(\\n            name=\"tags\",\\n            display_name=\"Tags\",\\n            info=\"Comma-separated list of tags to add to the run trace.\",\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"stop_tokens\",\\n            display_name=\"Stop Tokens\",\\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\n            name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True\\n        ),\\n        BoolInput(\\n            name=\"tool_model_enabled\",\\n            display_name=\"Tool Model Enabled\",\\n            info=\"Whether to enable tool calling in the model.\",\\n            value=False,\\n            real_time_refresh=True,\\n        ),\\n        MessageTextInput(\\n            name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True\\n        ),\\n        *LCModelComponent._base_inputs,\\n    ]\\n\\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\\n        # Mapping mirostat settings to their corresponding values\\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\\n\\n        # Default to 0 for \\'Disabled\\'\\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\\n\\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\\n        if mirostat_value == 0:\\n            mirostat_eta = None\\n            mirostat_tau = None\\n        else:\\n            mirostat_eta = self.mirostat_eta\\n            mirostat_tau = self.mirostat_tau\\n\\n        # Mapping system settings to their corresponding values\\n        llm_params = {\\n            \"base_url\": self.base_url,\\n            \"model\": self.model_name,\\n            \"mirostat\": mirostat_value,\\n            \"format\": self.format,\\n            \"metadata\": self.metadata,\\n            \"tags\": self.tags.split(\",\") if self.tags else None,\\n            \"mirostat_eta\": mirostat_eta,\\n            \"mirostat_tau\": mirostat_tau,\\n            \"num_ctx\": self.num_ctx or None,\\n            \"num_gpu\": self.num_gpu or None,\\n            \"num_thread\": self.num_thread or None,\\n            \"repeat_last_n\": self.repeat_last_n or None,\\n            \"repeat_penalty\": self.repeat_penalty or None,\\n            \"temperature\": self.temperature or None,\\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\\n            \"system\": self.system,\\n            \"tfs_z\": self.tfs_z or None,\\n            \"timeout\": self.timeout or None,\\n            \"top_k\": self.top_k or None,\\n            \"top_p\": self.top_p or None,\\n            \"verbose\": self.verbose,\\n            \"template\": self.template,\\n        }\\n\\n        # Remove parameters with None values\\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\\n\\n        try:\\n            output = ChatOllama(**llm_params)\\n        except Exception as e:\\n            msg = (\\n                \"Unable to connect to the Ollama API. \",\\n                \"Please verify the base URL, ensure the relevant Ollama model is pulled, and try again.\",\\n            )\\n            raise ValueError(msg) from e\\n\\n        return output\\n\\n    async def is_valid_ollama_url(self, url: str) -> bool:\\n        try:\\n            async with httpx.AsyncClient() as client:\\n                return (await client.get(urljoin(url, \"api/tags\"))).status_code == HTTP_STATUS_OK\\n        except httpx.RequestError:\\n            return False\\n\\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\\n        if field_name == \"mirostat\":\\n            if field_value == \"Disabled\":\\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\\n                build_config[\"mirostat_eta\"][\"value\"] = None\\n                build_config[\"mirostat_tau\"][\"value\"] = None\\n\\n            else:\\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\\n\\n                if field_value == \"Mirostat 2.0\":\\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\\n                else:\\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\\n\\n        if field_name in {\"base_url\", \"model_name\"} and not await self.is_valid_ollama_url(\\n            build_config[\"base_url\"].get(\"value\", \"\")\\n        ):\\n            # Check if any URL in the list is valid\\n            valid_url = \"\"\\n            for url in URL_LIST:\\n                if await self.is_valid_ollama_url(url):\\n                    valid_url = url\\n                    break\\n            if valid_url != \"\":\\n                build_config[\"base_url\"][\"value\"] = valid_url\\n            else:\\n                msg = \"No valid Ollama URL found.\"\\n                raise ValueError(msg)\\n        if field_name in {\"model_name\", \"base_url\", \"tool_model_enabled\"}:\\n            if await self.is_valid_ollama_url(self.base_url):\\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\\n                build_config[\"model_name\"][\"options\"] = await self.get_model(self.base_url, tool_model_enabled)\\n            elif await self.is_valid_ollama_url(build_config[\"base_url\"].get(\"value\", \"\")):\\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\\n                build_config[\"model_name\"][\"options\"] = await self.get_model(\\n                    build_config[\"base_url\"].get(\"value\", \"\"), tool_model_enabled\\n                )\\n            else:\\n                build_config[\"model_name\"][\"options\"] = []\\n        if field_name == \"keep_alive_flag\":\\n            if field_value == \"Keep\":\\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\\n                build_config[\"keep_alive\"][\"advanced\"] = True\\n            elif field_value == \"Immediately\":\\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\\n                build_config[\"keep_alive\"][\"advanced\"] = True\\n            else:\\n                build_config[\"keep_alive\"][\"advanced\"] = False\\n\\n        return build_config\\n\\n    async def get_model(self, base_url_value: str, tool_model_enabled: bool | None = None) -> list[str]:\\n        try:\\n            url = urljoin(base_url_value, \"api/tags\")\\n            async with httpx.AsyncClient() as client:\\n                response = await client.get(url)\\n                response.raise_for_status()\\n                data = response.json()\\n\\n            model_ids = [model[\"name\"] for model in data.get(\"models\", [])]\\n            # this to ensure that not embedding models are included.\\n            # not even the base models since models can have 1b 2b etc\\n            # handles cases when embeddings models have tags like :latest - etc.\\n            model_ids = [\\n                model\\n                for model in model_ids\\n                if not any(\\n                    model == embedding_model or model.startswith(embedding_model.split(\"-\")[0])\\n                    for embedding_model in OLLAMA_EMBEDDING_MODELS\\n                )\\n            ]\\n\\n        except (ImportError, ValueError, httpx.RequestError, Exception) as e:\\n            msg = \"Could not get model names from Ollama.\"\\n            raise ValueError(msg) from e\\n        return (\\n            model_ids if not tool_model_enabled else [model for model in model_ids if self.supports_tool_calling(model)]\\n        )\\n\\n    def supports_tool_calling(self, model: str) -> bool:\\n        \"\"\"Check if model name is in the base of any models example llama3.3 can have 1b and 2b.\"\"\"\\n        return any(model.startswith(f\"{tool_model}\") for tool_model in OLLAMA_TOOL_MODELS_BASE)\\n',\n",
       "        'fileTypes': [],\n",
       "        'file_path': '',\n",
       "        'password': False,\n",
       "        'name': 'code',\n",
       "        'advanced': True,\n",
       "        'dynamic': True,\n",
       "        'info': '',\n",
       "        'load_from_db': False,\n",
       "        'title_case': False},\n",
       "       'format': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'format',\n",
       "        'value': '',\n",
       "        'display_name': 'Format',\n",
       "        'advanced': True,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'Specify the format of the output (e.g., json).',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'input_value': {'trace_as_input': True,\n",
       "        'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'input_value',\n",
       "        'value': '',\n",
       "        'display_name': 'Input',\n",
       "        'advanced': False,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': '',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageInput'},\n",
       "       'metadata': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'metadata',\n",
       "        'value': {},\n",
       "        'display_name': 'Metadata',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Metadata to add to the run trace.',\n",
       "        'title_case': False,\n",
       "        'type': 'dict',\n",
       "        '_input_type': 'DictInput'},\n",
       "       'mirostat': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'options': ['Disabled', 'Mirostat', 'Mirostat 2.0'],\n",
       "        'options_metadata': [],\n",
       "        'combobox': False,\n",
       "        'dialog_inputs': {},\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'mirostat',\n",
       "        'value': 'Disabled',\n",
       "        'display_name': 'Mirostat',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Enable/disable Mirostat sampling for controlling perplexity.',\n",
       "        'real_time_refresh': True,\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'DropdownInput'},\n",
       "       'mirostat_eta': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'mirostat_eta',\n",
       "        'value': '',\n",
       "        'display_name': 'Mirostat Eta',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Learning rate for Mirostat algorithm. (Default: 0.1)',\n",
       "        'title_case': False,\n",
       "        'type': 'float',\n",
       "        '_input_type': 'FloatInput'},\n",
       "       'mirostat_tau': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'mirostat_tau',\n",
       "        'value': '',\n",
       "        'display_name': 'Mirostat Tau',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Controls the balance between coherence and diversity of the output. (Default: 5.0)',\n",
       "        'title_case': False,\n",
       "        'type': 'float',\n",
       "        '_input_type': 'FloatInput'},\n",
       "       'model_name': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'options': ['deepseek-r1:8b',\n",
       "         'Lucie:latest',\n",
       "         'llama3.2-vision:latest',\n",
       "         'mistral-nemo:latest',\n",
       "         'gemma2:latest',\n",
       "         'llama3.2:3b',\n",
       "         'llama3.2:1b',\n",
       "         'phi3.5:latest',\n",
       "         'llama3.1:8b'],\n",
       "        'options_metadata': [],\n",
       "        'combobox': False,\n",
       "        'dialog_inputs': {},\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'model_name',\n",
       "        'value': 'llama3.2:3b',\n",
       "        'display_name': 'Model Name',\n",
       "        'advanced': False,\n",
       "        'dynamic': False,\n",
       "        'info': 'Refer to https://ollama.com/library for more models.',\n",
       "        'real_time_refresh': True,\n",
       "        'refresh_button': True,\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'DropdownInput'},\n",
       "       'num_ctx': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'num_ctx',\n",
       "        'value': '',\n",
       "        'display_name': 'Context Window Size',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Size of the context window for generating tokens. (Default: 2048)',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'num_gpu': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'num_gpu',\n",
       "        'value': '',\n",
       "        'display_name': 'Number of GPUs',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'num_thread': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'num_thread',\n",
       "        'value': '',\n",
       "        'display_name': 'Number of Threads',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Number of threads to use during computation. (Default: detected for optimal performance)',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'repeat_last_n': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'repeat_last_n',\n",
       "        'value': '',\n",
       "        'display_name': 'Repeat Last N',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'repeat_penalty': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'repeat_penalty',\n",
       "        'value': '',\n",
       "        'display_name': 'Repeat Penalty',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Penalty for repetitions in generated text. (Default: 1.1)',\n",
       "        'title_case': False,\n",
       "        'type': 'float',\n",
       "        '_input_type': 'FloatInput'},\n",
       "       'stop_tokens': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'stop_tokens',\n",
       "        'value': '',\n",
       "        'display_name': 'Stop Tokens',\n",
       "        'advanced': True,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'Comma-separated list of tokens to signal the model to stop generating text.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'stream': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'stream',\n",
       "        'value': True,\n",
       "        'display_name': 'Stream',\n",
       "        'advanced': False,\n",
       "        'dynamic': False,\n",
       "        'info': 'Stream the response from the model. Streaming works only in Chat.',\n",
       "        'title_case': False,\n",
       "        'type': 'bool',\n",
       "        '_input_type': 'BoolInput'},\n",
       "       'system': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'system',\n",
       "        'value': '',\n",
       "        'display_name': 'System',\n",
       "        'advanced': True,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'System to use for generating text.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'system_message': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'multiline': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'system_message',\n",
       "        'value': '',\n",
       "        'display_name': 'System Message',\n",
       "        'advanced': False,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'System message to pass to the model.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MultilineInput'},\n",
       "       'tags': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'tags',\n",
       "        'value': '',\n",
       "        'display_name': 'Tags',\n",
       "        'advanced': True,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'Comma-separated list of tags to add to the run trace.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'temperature': {'tool_mode': False,\n",
       "        'min_label': '',\n",
       "        'max_label': '',\n",
       "        'min_label_icon': '',\n",
       "        'max_label_icon': '',\n",
       "        'slider_buttons': False,\n",
       "        'slider_buttons_options': [],\n",
       "        'slider_input': False,\n",
       "        'range_spec': {'step_type': 'float', 'min': 0, 'max': 1, 'step': 0.01},\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'temperature',\n",
       "        'value': 0.2,\n",
       "        'display_name': 'Temperature',\n",
       "        'advanced': False,\n",
       "        'dynamic': False,\n",
       "        'info': '',\n",
       "        'title_case': False,\n",
       "        'type': 'slider',\n",
       "        '_input_type': 'SliderInput'},\n",
       "       'template': {'tool_mode': False,\n",
       "        'trace_as_input': True,\n",
       "        'trace_as_metadata': True,\n",
       "        'load_from_db': False,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'template',\n",
       "        'value': '',\n",
       "        'display_name': 'Template',\n",
       "        'advanced': True,\n",
       "        'input_types': ['Message'],\n",
       "        'dynamic': False,\n",
       "        'info': 'Template to use for generating text.',\n",
       "        'title_case': False,\n",
       "        'type': 'str',\n",
       "        '_input_type': 'MessageTextInput'},\n",
       "       'tfs_z': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'tfs_z',\n",
       "        'value': '',\n",
       "        'display_name': 'TFS Z',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Tail free sampling value. (Default: 1)',\n",
       "        'title_case': False,\n",
       "        'type': 'float',\n",
       "        '_input_type': 'FloatInput'},\n",
       "       'timeout': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'timeout',\n",
       "        'value': '',\n",
       "        'display_name': 'Timeout',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Timeout for the request stream.',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'tool_model_enabled': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'tool_model_enabled',\n",
       "        'value': False,\n",
       "        'display_name': 'Tool Model Enabled',\n",
       "        'advanced': False,\n",
       "        'dynamic': False,\n",
       "        'info': 'Whether to enable tool calling in the model.',\n",
       "        'real_time_refresh': True,\n",
       "        'title_case': False,\n",
       "        'type': 'bool',\n",
       "        '_input_type': 'BoolInput'},\n",
       "       'top_k': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'top_k',\n",
       "        'value': '',\n",
       "        'display_name': 'Top K',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Limits token selection to top K. (Default: 40)',\n",
       "        'title_case': False,\n",
       "        'type': 'int',\n",
       "        '_input_type': 'IntInput'},\n",
       "       'top_p': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'top_p',\n",
       "        'value': '',\n",
       "        'display_name': 'Top P',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Works together with top-k. (Default: 0.9)',\n",
       "        'title_case': False,\n",
       "        'type': 'float',\n",
       "        '_input_type': 'FloatInput'},\n",
       "       'verbose': {'tool_mode': False,\n",
       "        'trace_as_metadata': True,\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'verbose',\n",
       "        'value': False,\n",
       "        'display_name': 'Verbose',\n",
       "        'advanced': True,\n",
       "        'dynamic': False,\n",
       "        'info': 'Whether to print out response text.',\n",
       "        'title_case': False,\n",
       "        'type': 'bool',\n",
       "        '_input_type': 'BoolInput'}},\n",
       "      'description': 'Generate text using Ollama Local LLMs.',\n",
       "      'icon': 'Ollama',\n",
       "      'base_classes': ['LanguageModel', 'Message'],\n",
       "      'display_name': 'Ollama',\n",
       "      'documentation': '',\n",
       "      'minimized': False,\n",
       "      'custom_fields': {},\n",
       "      'output_types': [],\n",
       "      'pinned': False,\n",
       "      'conditional_paths': [],\n",
       "      'frozen': False,\n",
       "      'outputs': [{'types': ['Message'],\n",
       "        'selected': 'Message',\n",
       "        'name': 'text_output',\n",
       "        'hidden': None,\n",
       "        'display_name': 'Message',\n",
       "        'method': 'text_response',\n",
       "        'value': '__UNDEFINED__',\n",
       "        'cache': True,\n",
       "        'required_inputs': [],\n",
       "        'allows_loop': False},\n",
       "       {'types': ['LanguageModel'],\n",
       "        'selected': 'LanguageModel',\n",
       "        'name': 'model_output',\n",
       "        'hidden': None,\n",
       "        'display_name': 'Language Model',\n",
       "        'method': 'build_model',\n",
       "        'value': '__UNDEFINED__',\n",
       "        'cache': True,\n",
       "        'required_inputs': [],\n",
       "        'allows_loop': False}],\n",
       "      'field_order': ['base_url',\n",
       "       'model_name',\n",
       "       'temperature',\n",
       "       'format',\n",
       "       'metadata',\n",
       "       'mirostat',\n",
       "       'mirostat_eta',\n",
       "       'mirostat_tau',\n",
       "       'num_ctx',\n",
       "       'num_gpu',\n",
       "       'num_thread',\n",
       "       'repeat_last_n',\n",
       "       'repeat_penalty',\n",
       "       'tfs_z',\n",
       "       'timeout',\n",
       "       'top_k',\n",
       "       'top_p',\n",
       "       'verbose',\n",
       "       'tags',\n",
       "       'stop_tokens',\n",
       "       'system',\n",
       "       'tool_model_enabled',\n",
       "       'template',\n",
       "       'input_value',\n",
       "       'system_message',\n",
       "       'stream'],\n",
       "      'beta': False,\n",
       "      'legacy': False,\n",
       "      'edited': False,\n",
       "      'metadata': {},\n",
       "      'tool_mode': False,\n",
       "      'category': 'models',\n",
       "      'key': 'OllamaModel',\n",
       "      'score': 1.1514907767935298e-16,\n",
       "      'lf_version': '1.1.3'},\n",
       "     'showNode': True,\n",
       "     'type': 'OllamaModel',\n",
       "     'id': 'OllamaModel-JEWib'},\n",
       "    'selected': False,\n",
       "    'measured': {'width': 320, 'height': 696},\n",
       "    'dragging': False},\n",
       "   {'id': 'DemarchesSimplifieesOCR-L5RHF',\n",
       "    'type': 'genericNode',\n",
       "    'position': {'x': 461.91819568811627, 'y': -319.7842956232768},\n",
       "    'data': {'node': {'template': {'_type': 'Component',\n",
       "       'document': {'trace_as_metadata': True,\n",
       "        'file_path': '',\n",
       "        'fileTypes': ['pdf', 'jpg', 'png'],\n",
       "        'list': False,\n",
       "        'list_add_label': 'Add More',\n",
       "        'required': False,\n",
       "        'placeholder': '',\n",
       "        'show': True,\n",
       "        'name': 'document',\n",
       "        'value': '',\n",
       "        'display_name': 'Document',\n",
       "        'advanced': False,\n",
       "        'dynamic': False,\n",
       "        'info': '',\n",
       "        'title_case': False,\n",
       "        'type': 'file',\n",
       "        '_input_type': 'FileInput'},\n",
       "       'code': {'type': 'code',\n",
       "        'required': True,\n",
       "        'placeholder': '',\n",
       "        'list': False,\n",
       "        'show': True,\n",
       "        'multiline': True,\n",
       "        'value': 'import os\\nfrom langflow.custom import Component\\nfrom langflow.io import HandleInput, MessageTextInput, Output, FileInput\\nfrom langflow.schema.message import Message\\nfrom langflow.schema import Data\\nfrom pdf2image import convert_from_path\\nfrom langflow.field_typing import Text\\n\\nimport requests\\nimport torch\\nfrom transformers import pipeline\\nfrom transformers import AutoProcessor, AutoModelForCausalLM \\nfrom PIL import Image\\n\\n# optimize with fp16 loading\\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True, torch_dtype=torch.float16).to(torch.cuda.current_device())\\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\\n\\ndef run_ocr(image):\\n    task = \"<OCR>\"\\n    response = run_florence_2(image, task, text_input=\"\")\\n    return response.get(task, \"\")\\n\\ndef run_florence_2(image, task_prompt, text_input=None):\\n    if text_input is None:\\n        prompt = task_prompt\\n    else:\\n        prompt = task_prompt + text_input\\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.cuda.current_device(), dtype=torch.float16)\\n    generated_ids = model.generate(\\n      input_ids=inputs[\"input_ids\"],\\n      pixel_values=inputs[\"pixel_values\"],\\n      max_new_tokens=128,\\n      num_beams=1\\n    )\\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\\n\\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\\n    return parsed_answer\\n\\n\\nclass DemarchesSimplifieesOCR(Component):\\n    display_name = \"OCR\"\\n    description = \"Convert file to OCR\"\\n    icon = \"custom_components\"\\n    name = \"DemarchesSimplifieesOCR\"\\n\\n    inputs = [\\n        FileInput(name=\"document\", display_name=\"Document\", file_types=[\"pdf\", \"jpg\", \"png\"]),\\n    ]\\n\\n    outputs = [\\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\\n    ]\\n\\n    def build_output(self) -> Message:\\n        print(type(self.document), self.document)\\n        captions = []\\n\\n        if self.document.lower().endswith(\\'.pdf\\'):\\n            images = convert_from_path(self.document)\\n            for i, img in enumerate(images):\\n                path = \\'page\\'+ str(i) +\\'.jpg\\'\\n                img.save(path, \\'JPEG\\')\\n                image = Image.open(path)\\n                caption = run_ocr(image)\\n                captions.append(caption)\\n                os.remove(path)\\n        else:\\n            image = Image.open(self.document)\\n            caption = run_ocr(image)\\n            captions.append(caption)\\n            \\n        ocr_output = \"\\\\n\".join(captions)\\n        return Message(\\n            text=ocr_output,\\n        )',\n",
       "        'fileTypes': [],\n",
       "        'file_path': '',\n",
       "        'password': False,\n",
       "        'name': 'code',\n",
       "        'advanced': True,\n",
       "        'dynamic': True,\n",
       "        'info': '',\n",
       "        'load_from_db': False,\n",
       "        'title_case': False}},\n",
       "      'description': 'Convert file to OCR',\n",
       "      'icon': 'custom_components',\n",
       "      'base_classes': ['Message'],\n",
       "      'display_name': 'OCR',\n",
       "      'documentation': '',\n",
       "      'minimized': False,\n",
       "      'custom_fields': {},\n",
       "      'output_types': [],\n",
       "      'pinned': False,\n",
       "      'conditional_paths': [],\n",
       "      'frozen': False,\n",
       "      'outputs': [{'types': ['Message'],\n",
       "        'selected': 'Message',\n",
       "        'name': 'output',\n",
       "        'hidden': None,\n",
       "        'display_name': 'Output',\n",
       "        'method': 'build_output',\n",
       "        'value': '__UNDEFINED__',\n",
       "        'cache': True,\n",
       "        'required_inputs': None,\n",
       "        'allows_loop': False}],\n",
       "      'field_order': ['document'],\n",
       "      'beta': False,\n",
       "      'legacy': False,\n",
       "      'edited': False,\n",
       "      'metadata': {},\n",
       "      'tool_mode': False},\n",
       "     'showNode': True,\n",
       "     'type': 'DemarchesSimplifieesOCR',\n",
       "     'id': 'DemarchesSimplifieesOCR-L5RHF',\n",
       "     'description': 'Convert file to OCR',\n",
       "     'display_name': 'OCR'},\n",
       "    'selected': False,\n",
       "    'measured': {'width': 320, 'height': 226},\n",
       "    'dragging': False}],\n",
       "  'edges': [{'source': 'OllamaModel-JEWib',\n",
       "    'sourceHandle': '{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-JEWibœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}',\n",
       "    'target': 'ChatOutput-ECs31',\n",
       "    'targetHandle': '{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ECs31œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'data': {'targetHandle': {'fieldName': 'input_value',\n",
       "      'id': 'ChatOutput-ECs31',\n",
       "      'inputTypes': ['Message'],\n",
       "      'type': 'str'},\n",
       "     'sourceHandle': {'dataType': 'OllamaModel',\n",
       "      'id': 'OllamaModel-JEWib',\n",
       "      'name': 'text_output',\n",
       "      'output_types': ['Message']}},\n",
       "    'id': 'xy-edge__OllamaModel-JEWib{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-JEWibœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ECs31{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ECs31œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'animated': False,\n",
       "    'className': ''},\n",
       "   {'source': 'Prompt-71BBi',\n",
       "    'sourceHandle': '{œdataTypeœ:œPromptœ,œidœ:œPrompt-71BBiœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}',\n",
       "    'target': 'OllamaModel-JEWib',\n",
       "    'targetHandle': '{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-JEWibœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'data': {'targetHandle': {'fieldName': 'input_value',\n",
       "      'id': 'OllamaModel-JEWib',\n",
       "      'inputTypes': ['Message'],\n",
       "      'type': 'str'},\n",
       "     'sourceHandle': {'dataType': 'Prompt',\n",
       "      'id': 'Prompt-71BBi',\n",
       "      'name': 'prompt',\n",
       "      'output_types': ['Message']}},\n",
       "    'id': 'xy-edge__Prompt-71BBi{œdataTypeœ:œPromptœ,œidœ:œPrompt-71BBiœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-JEWib{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-JEWibœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'animated': False,\n",
       "    'className': ''},\n",
       "   {'source': 'DemarchesSimplifieesOCR-L5RHF',\n",
       "    'sourceHandle': '{œdataTypeœ:œDemarchesSimplifieesOCRœ,œidœ:œDemarchesSimplifieesOCR-L5RHFœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}',\n",
       "    'target': 'Prompt-71BBi',\n",
       "    'targetHandle': '{œfieldNameœ:œdocumentœ,œidœ:œPrompt-71BBiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'data': {'targetHandle': {'fieldName': 'document',\n",
       "      'id': 'Prompt-71BBi',\n",
       "      'inputTypes': ['Message'],\n",
       "      'type': 'str'},\n",
       "     'sourceHandle': {'dataType': 'DemarchesSimplifieesOCR',\n",
       "      'id': 'DemarchesSimplifieesOCR-L5RHF',\n",
       "      'name': 'output',\n",
       "      'output_types': ['Message']}},\n",
       "    'id': 'xy-edge__DemarchesSimplifieesOCR-L5RHF{œdataTypeœ:œDemarchesSimplifieesOCRœ,œidœ:œDemarchesSimplifieesOCR-L5RHFœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-Prompt-71BBi{œfieldNameœ:œdocumentœ,œidœ:œPrompt-71BBiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}',\n",
       "    'className': ''}],\n",
       "  'viewport': {'x': -275.47424651417145,\n",
       "   'y': 528.9538613311092,\n",
       "   'zoom': 0.7090263270243259}},\n",
       " 'is_component': False,\n",
       " 'updated_at': '2025-02-05T15:51:35+00:00',\n",
       " 'webhook': False,\n",
       " 'endpoint_name': None,\n",
       " 'tags': None,\n",
       " 'locked': False,\n",
       " 'id': 'f71bcc8b-9cc0-455e-9ddc-15811164b8a3',\n",
       " 'user_id': '3f2770a9-66d5-4f18-afbc-59de6cb8f75f',\n",
       " 'folder_id': '6186a96b-c6e0-4109-8dcf-5470965ee63f'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = get_flow(OLLAMA_FLOW)\n",
    "flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
